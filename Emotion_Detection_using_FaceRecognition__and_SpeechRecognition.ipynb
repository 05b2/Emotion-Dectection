{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjhTPtkjCZq8nwn+Vn109p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/05b2/Emotion-Dectection/blob/main/Emotion_Detection_using_FaceRecognition__and_SpeechRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emotion Detection using FACE Recognition\n",
        "Here we get input form user using Gradio Interface."
      ],
      "metadata": {
        "id": "QCYRks8MlCEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwTh-Tc22OHh",
        "outputId": "8f4c2fc9-ff12-400f-87a2-4656b4351b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "# Load face detection and emotion model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "emotion_model = load_model(\"emotion_model.h5\", compile=False)\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Prediction function\n",
        "def detect_emotion(image):\n",
        "    img = np.array(image)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        roi = gray[y:y+h, x:x+w]\n",
        "        roi = cv2.resize(roi, (64, 64))\n",
        "        roi = roi.astype(\"float32\") / 255.0\n",
        "        roi = np.expand_dims(roi, axis=-1)\n",
        "        roi = np.expand_dims(roi, axis=0)\n",
        "\n",
        "        preds = emotion_model.predict(roi, verbose=0)\n",
        "        label = emotion_labels[np.argmax(preds)]\n",
        "\n",
        "        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        cv2.putText(img, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    return img\n",
        "\n",
        "# Gradio interface\n",
        "face_interface = gr.Interface(\n",
        "    fn=detect_emotion,\n",
        "    inputs=gr.Image(type='pil'),\n",
        "    outputs=gr.Image(type='numpy'),\n",
        "    title=\"Face Emotion Detection\",\n",
        "    description=\"Upload an image with a face to detect the emotion.\"\n",
        ")\n",
        "face_interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "cgP4APgr18rS",
        "outputId": "6a2a04d5-0fba-4ed0-cc21-ca06c6c45bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b0ff400b119a2db3db.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b0ff400b119a2db3db.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detecting emotion Using Speech Recognition"
      ],
      "metadata": {
        "id": "lKzK29pZqq-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai-whisper librosa scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOFkn6uwqqVT",
        "outputId": "c12660cf-999f-4499-8a9f-414a699032bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install openai-whisper librosa scikit-learn\n",
        "import whisper\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from moviepy.editor import AudioFileClip\n",
        "\n",
        "# Step 1: Load Whisper Model for Speech Recognition\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Step 2: Transcribe Speech using Whisper\n",
        "def transcribe_audio(audio_path):\n",
        "    result = model.transcribe(audio_path)\n",
        "    print(\"Transcription: \", result[\"text\"])\n",
        "    return result[\"text\"], result[\"language\"]\n",
        "\n",
        "# Step 3: Extract Audio Features (MFCCs and Pitch) for Emotion Recognition\n",
        "def extract_audio_features(audio_path):\n",
        "    # Load audio using librosa\n",
        "    y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Extract MFCC features\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "    # Extract pitch\n",
        "    pitch, _ = librosa.core.piptrack(y=y, sr=sr)\n",
        "\n",
        "    # Ensure pitch has same length as MFCC (take mean of pitch over time)\n",
        "    pitch = np.mean(pitch, axis=1)\n",
        "\n",
        "    # Calculate mean and standard deviation for MFCCs and pitch as features\n",
        "    mfcc_features = np.mean(mfcc, axis=1)\n",
        "\n",
        "    # Limit pitch features to match number of MFCC features (13 features)\n",
        "    pitch_features = pitch[:13]  # Use first 13 pitch features\n",
        "\n",
        "    # Combine features\n",
        "    audio_features = np.concatenate([mfcc_features, pitch_features])\n",
        "\n",
        "    return audio_features\n",
        "\n",
        "# Step 4: Train a Simple Emotion Recognition Model (You can train this model beforehand with labeled data)\n",
        "def train_emotion_classifier():\n",
        "    # Sample labeled data (features and emotions)\n",
        "    X = np.random.rand(100, 26)  # 13 MFCC + 13 pitch features\n",
        "    y = np.random.choice(['happy', 'sad', 'angry', 'neutral'], size=100)\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "\n",
        "    # Train a simple classifier (Support Vector Machine)\n",
        "    classifier = SVC(kernel='linear')\n",
        "    classifier.fit(X, y_encoded)\n",
        "\n",
        "    return classifier, le\n",
        "\n",
        "# Step 5: Classify Emotion Based on Extracted Features\n",
        "def classify_emotion(features, classifier, le):\n",
        "    emotion_idx = classifier.predict([features])[0]\n",
        "    emotion = le.inverse_transform([emotion_idx])[0]\n",
        "    return emotion\n",
        "\n",
        "# Step 6: Convert MP4 to WAV using MoviePy\n",
        "def convert_mp4_to_wav(mp4_path, wav_path):\n",
        "    audio_clip = AudioFileClip(mp4_path)\n",
        "    audio_clip.write_audiofile(wav_path, codec='pcm_s16le')\n",
        "\n",
        "# Step 7: Combine Everything for Emotion-Aware Speech Recognition\n",
        "def emotion_aware_speech_recognition(mp4_path):\n",
        "    # Step 7.1: Convert MP4 to WAV\n",
        "    wav_path = \"/content/temp_audio.wav\"\n",
        "    convert_mp4_to_wav(mp4_path, wav_path)\n",
        "\n",
        "    # Step 7.2: Transcribe Audio\n",
        "    transcription, language = transcribe_audio(wav_path)\n",
        "\n",
        "    # Step 7.3: Extract Audio Features for Emotion Detection\n",
        "    audio_features = extract_audio_features(wav_path)\n",
        "\n",
        "    # Step 7.4: Classify Emotion Based on Audio Features\n",
        "    emotion = classify_emotion(audio_features, emotion_classifier, label_encoder)\n",
        "\n",
        "    # Step 7.5: Output Result\n",
        "    print(f\"Detected Emotion: {emotion}\")\n",
        "    print(f\"Transcription: {transcription}\")\n",
        "    print(f\"Language Detected: {language}\")\n",
        "\n",
        "# Step 8: Train the Emotion Classifier (Only once)\n",
        "emotion_classifier, label_encoder = train_emotion_classifier()\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/WhatsApp  9.10.45 PM.ogg\"  # Replace with your actual file path\n",
        "emotion_aware_speech_recognition(audio_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft30YG0utg9g",
        "outputId": "2ec84089-9423-4a88-d8f0-993ce841d8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in /content/temp_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Transcription:   Oh this splar looks so beautiful.\n",
            "Detected Emotion: angry\n",
            "Transcription:  Oh this splar looks so beautiful.\n",
            "Language Detected: en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from moviepy.editor import AudioFileClip\n",
        "\n",
        "# Step 1: Load Whisper Model for Speech Recognition\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Step 2: Transcribe Speech using Whisper\n",
        "def transcribe_audio(audio_path):\n",
        "    result = model.transcribe(audio_path)\n",
        "    print(\"Transcription: \", result[\"text\"])\n",
        "    return result[\"text\"], result[\"language\"]\n",
        "\n",
        "# Step 3: Extract Audio Features (MFCCs and Pitch) for Emotion Recognition\n",
        "def extract_audio_features(audio_path):\n",
        "    # Load audio using librosa\n",
        "    y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Extract MFCC features\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "    # Extract pitch\n",
        "    pitch, _ = librosa.core.piptrack(y=y, sr=sr)\n",
        "\n",
        "    # Ensure pitch has same length as MFCC (take mean of pitch over time)\n",
        "    pitch = np.mean(pitch, axis=1)\n",
        "\n",
        "    # Calculate mean and standard deviation for MFCCs and pitch as features\n",
        "    mfcc_features = np.mean(mfcc, axis=1)\n",
        "\n",
        "    # Limit pitch features to match number of MFCC features (13 features)\n",
        "    pitch_features = pitch[:13]  # Use first 13 pitch features\n",
        "\n",
        "    # Combine features\n",
        "    audio_features = np.concatenate([mfcc_features, pitch_features])\n",
        "\n",
        "    return audio_features\n",
        "\n",
        "# Step 4: Train a Simple Emotion Recognition Model (You can train this model beforehand with labeled data)\n",
        "def train_emotion_classifier():\n",
        "    # Sample labeled data (features and emotions)\n",
        "    X = np.random.rand(100, 26)  # 13 MFCC + 13 pitch features\n",
        "    y = np.random.choice(['happy', 'sad', 'angry', 'neutral'], size=100)\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "\n",
        "    # Train a simple classifier (Support Vector Machine)\n",
        "    classifier = SVC(kernel='linear')\n",
        "    classifier.fit(X, y_encoded)\n",
        "\n",
        "    return classifier, le\n",
        "\n",
        "# Step 5: Classify Emotion Based on Extracted Features\n",
        "def classify_emotion(features, classifier, le):\n",
        "    emotion_idx = classifier.predict([features])[0]\n",
        "    emotion = le.inverse_transform([emotion_idx])[0]\n",
        "    return emotion\n",
        "\n",
        "# Step 6: Convert MP4 to WAV using MoviePy\n",
        "def convert_mp4_to_wav(mp4_path, wav_path):\n",
        "    audio_clip = AudioFileClip(mp4_path)\n",
        "    audio_clip.write_audiofile(wav_path, codec='pcm_s16le')\n",
        "\n",
        "# Step 7: Combine Everything for Emotion-Aware Speech Recognition\n",
        "def emotion_aware_speech_recognition(mp4_path):\n",
        "    # Step 7.1: Convert MP4 to WAV\n",
        "    wav_path = \"/content/temp_audio.wav\"\n",
        "    convert_mp4_to_wav(mp4_path, wav_path)\n",
        "\n",
        "    # Step 7.2: Transcribe Audio\n",
        "    transcription, language = transcribe_audio(wav_path)\n",
        "\n",
        "    # Step 7.3: Extract Audio Features for Emotion Detection\n",
        "    audio_features = extract_audio_features(wav_path)\n",
        "\n",
        "    # Step 7.4: Classify Emotion Based on Audio Features\n",
        "    emotion = classify_emotion(audio_features, emotion_classifier, label_encoder)\n",
        "\n",
        "    # Step 7.5: Output Result\n",
        "    print(f\"Detected Emotion: {emotion}\")\n",
        "    print(f\"Transcription: {transcription}\")\n",
        "    print(f\"Language Detected: {language}\")\n",
        "\n",
        "# Step 8: Train the Emotion Classifier (Only once)\n",
        "emotion_classifier, label_encoder = train_emotion_classifier()\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/WhatsApp Ptt 2025-05-04 at 9.10.23 PM.ogg\"  # Replace with your actual file path\n",
        "emotion_aware_speech_recognition(audio_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0KJgMMYugm_",
        "outputId": "b0367940-9933-446a-8422-debc2b157743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in /content/temp_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Transcription:   Are you an idiot? Are you nuts? Have you gone mad?\n",
            "Detected Emotion: neutral\n",
            "Transcription:  Are you an idiot? Are you nuts? Have you gone mad?\n",
            "Language Detected: en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from moviepy.editor import AudioFileClip\n",
        "\n",
        "# Step 1: Load Whisper Model for Speech Recognition\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Step 2: Transcribe Speech using Whisper\n",
        "def transcribe_audio(audio_path):\n",
        "    result = model.transcribe(audio_path)\n",
        "    print(\"Transcription: \", result[\"text\"])\n",
        "    return result[\"text\"], result[\"language\"]\n",
        "\n",
        "# Step 3: Extract Audio Features (MFCCs and Pitch) for Emotion Recognition\n",
        "def extract_audio_features(audio_path):\n",
        "    # Load audio using librosa\n",
        "    y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Extract MFCC features\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "    # Extract pitch\n",
        "    pitch, _ = librosa.core.piptrack(y=y, sr=sr)\n",
        "\n",
        "    # Ensure pitch has same length as MFCC (take mean of pitch over time)\n",
        "    pitch = np.mean(pitch, axis=1)\n",
        "\n",
        "    # Calculate mean and standard deviation for MFCCs and pitch as features\n",
        "    mfcc_features = np.mean(mfcc, axis=1)\n",
        "\n",
        "    # Limit pitch features to match number of MFCC features (13 features)\n",
        "    pitch_features = pitch[:13]  # Use first 13 pitch features\n",
        "\n",
        "    # Combine features\n",
        "    audio_features = np.concatenate([mfcc_features, pitch_features])\n",
        "\n",
        "    return audio_features\n",
        "\n",
        "# Step 4: Train a Simple Emotion Recognition Model (You can train this model beforehand with labeled data)\n",
        "def train_emotion_classifier():\n",
        "    # Sample labeled data (features and emotions)\n",
        "    X = np.random.rand(100, 26)  # 13 MFCC + 13 pitch features\n",
        "    y = np.random.choice(['happy', 'sad', 'angry', 'neutral'], size=100)\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "\n",
        "    # Train a simple classifier (Support Vector Machine)\n",
        "    classifier = SVC(kernel='linear')\n",
        "    classifier.fit(X, y_encoded)\n",
        "\n",
        "    return classifier, le\n",
        "\n",
        "# Step 5: Classify Emotion Based on Extracted Features\n",
        "def classify_emotion(features, classifier, le):\n",
        "    emotion_idx = classifier.predict([features])[0]\n",
        "    emotion = le.inverse_transform([emotion_idx])[0]\n",
        "    return emotion\n",
        "\n",
        "# Step 6: Convert MP4 to WAV using MoviePy\n",
        "def convert_mp4_to_wav(mp4_path, wav_path):\n",
        "    audio_clip = AudioFileClip(mp4_path)\n",
        "    audio_clip.write_audiofile(wav_path, codec='pcm_s16le')\n",
        "\n",
        "# Step 7: Combine Everything for Emotion-Aware Speech Recognition\n",
        "def emotion_aware_speech_recognition(mp4_path):\n",
        "    # Step 7.1: Convert MP4 to WAV\n",
        "    wav_path = \"/content/temp_audio.wav\"\n",
        "    convert_mp4_to_wav(mp4_path, wav_path)\n",
        "\n",
        "    # Step 7.2: Transcribe Audio\n",
        "    transcription, language = transcribe_audio(wav_path)\n",
        "\n",
        "    # Step 7.3: Extract Audio Features for Emotion Detection\n",
        "    audio_features = extract_audio_features(wav_path)\n",
        "\n",
        "    # Step 7.4: Classify Emotion Based on Audio Features\n",
        "    emotion = classify_emotion(audio_features, emotion_classifier, label_encoder)\n",
        "\n",
        "    # Step 7.5: Output Result\n",
        "    print(f\"Detected Emotion: {emotion}\")\n",
        "    print(f\"Transcription: {transcription}\")\n",
        "    print(f\"Language Detected: {language}\")\n",
        "\n",
        "# Step 8: Train the Emotion Classifier (Only once)\n",
        "emotion_classifier, label_encoder = train_emotion_classifier()\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/WhatsApp  2.32.00 PM.ogg\"  # Replace with your actual file path\n",
        "emotion_aware_speech_recognition(audio_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ts1XDYYvWIX",
        "outputId": "b689c2c5-914c-41dc-b5dc-fd11c54506fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in /content/temp_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Transcription:   There, how do you happy?\n",
            "Detected Emotion: neutral\n",
            "Transcription:  There, how do you happy?\n",
            "Language Detected: en\n"
          ]
        }
      ]
    }
  ]
}